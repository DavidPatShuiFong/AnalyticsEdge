---
title: "Text Analytics and Criminal Justice"
author: "David Fong"
date: "21st March 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tm)        # for text analytics
library(caTools)   # to create training/test sets
library(rpart)     # Classification and Regression Tree (CART)
library(rpart.plot)
library(ROCR)      # receiver operator characteristic curve
```

## data

Enron e-mails. Used in investigation where Enron is accused of distorting prices
in the California electricity market.

This data comes from the [2010 TREC Legal Track](http://trec-legal.umiacs.umd.edu/).

```{r}
emails <- read.csv("energy_bids.csv", stringsAsFactors = FALSE)
str(emails)
```

'responsive' e-mails are thought relevant for subsequent use in legal proceedings against Enron.

## a typical e-mail

```{r}
emails$email[[1]]

cat("\nResponsive : ", emails$responsive[[1]])
```

## responsive e-mails

```{r}
table(emails$responsive)
```

## pre-process e-mails for ease of analysis

```{r}
corpus <- VCorpus(VectorSource(emails$email))

corpus = tm_map(corpus, content_transformer(tolower)) # lower-case
corpus = tm_map(corpus, removePunctuation)            # remove punctuation
corpus = tm_map(corpus, removeWords, stopwords("english")) # remove stop words
corpus = tm_map(corpus, stemDocument)                 # reduce words to 'stems'

```

## bag of words
```{r}
dtm <- DocumentTermMatrix(corpus)
dtm
```

too many terms (22141)!

```{r}
dtm <- removeSparseTerms(dtm, 0.97)
# only keep words present in at least 3% of documents
dtm
```

Now just 788 terms.

```{r}
labeledterms <- as.data.frame(as.matrix(dtm))
labeledterms$responsive <- emails$responsive # our outcome variable
str(labeledterms)
```

## Create training and test set

```{r}
set.seed(144)

split <- sample.split(labeledterms$responsive, SplitRatio = 0.7)
train <- subset(labeledterms, split == TRUE)
test <- subset(labeledterms, split == FALSE)

```

## Classication and Regression Trees (CART) models

```{r}
emailCART <- rpart(responsive ~ .,
                   data = train,
                   method = "class")
prp(emailCART)
```

**Jeff Skillings** was the CEO of Enron.

### evaluate on test set

```{r}
predictCART <- predict(emailCART,
                       newdata = test)
predictCART[1:10,]
```

Left column is the predicted probability of the document being non-responsive.
Right column is the predicted probability of the document being responsive.

```{r}
predictCART.prob <- predictCART[,2]
tableCART <- table(test$responsive, predictCART.prob > .5)
tableCART

cat("\nAccuracy :", sum(diag(tableCART))/nrow(test))
cat("\nAccuracy of baseline model\n(e-mail is not responsive) :", sum(test$responsive == 0)/nrow(test))
```

However, as in most document retrieval applications,
there are uneven costs for different types of errors here.

Typically, a human will still have to manually review
all of the predicted responsive documents
to make sure they are actually responsive.

Therefore, if we have a false positive,
in which a non-responsive document is labeled
as responsive, the mistake translates
to a bit of additional work in the manual review
process but no further harm, since the manual review process
will remove this erroneous result.

But on the other hand, if we have a false negative,
in which a responsive document is labeled as non-responsive
by our model, we will miss the document entirely
in our predictive coding process.

Therefore, we're going to assign a higher cost to false negatives
than to false positives, which makes this a good time to look
at other cut-offs on our ROC curve.

### ROC curve

```{r}
predictROCR <- prediction(predictCART.prob, test$responsive)
perfROCR <- performance(predictROCR, "tpr", "fpr") # true-positive rate vs false-positive rate

plot(perfROCR, colorize=TRUE,
     print.cutoffs.at = seq(0, 0.2, 0.05), text.adj = c(-0.2, 1.7))

cat("\nAUC :", performance(predictROCR, "auc")@y.values[[1]])
```

Perhaps a threshold of around 0.15?

### Using a threshold of 0.15

```{r}
predictCART.prob <- predictCART[,2]
tableCART <- table(test$responsive, predictCART.prob > .15)
tableCART

cat("\nAccuracy :", sum(diag(tableCART))/nrow(test))
cat("\nAccuracy of baseline model\n(e-mail is not responsive) :", sum(test$responsive == 0)/nrow(test))
```